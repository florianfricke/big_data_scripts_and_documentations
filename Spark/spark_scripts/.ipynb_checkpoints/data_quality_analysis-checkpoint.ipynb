{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the Data Quality from historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the notebook, the following Python modules must be installed from the command line.\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "`pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() #necessary to find the local spark\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, BooleanType, TimestampType, StructField, StructType\n",
    "from pyspark.sql.functions import col, sum, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Before the files can be read, they must be transferred to the HDFS. (For procedure, see documentation / installation instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.json('hdfs://192.168.0.10:9000/user/hadoop/IotTestbed_Dateien/festo_sensor_data_couchdb.json') #mock data\n",
    "df = spark.read.json('hdfs://192.168.0.10:9000/user/hadoop/IotTestbed_Dateien/festo_sensor_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"iotData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_utc(date):\n",
    "    return dt.datetime.utcfromtimestamp(int(date)/1000).strftime('%Y-%m-%d %H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of records in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"no. rows: {0}, no. columns: {1}, no. columns nested: {2}\".format(\n",
    "    df.count(), len(df.columns), len(df.select(\"doc.*\").columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get time period\n",
    "from `1512746433260 - 2017-12-08 | 15:20:33`\n",
    "\n",
    "to `1520587748768 - 2018-03-09 | 09:29:08`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT doc.timestamp FROM iotData ORDER BY timestamp\").show(50, False)\n",
    "spark.sql(\"SELECT doc.timestamp FROM iotData ORDER BY timestamp DESC\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_date_utc(1520587748768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modules in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of modules\n",
    "spark.sql(\"SELECT DISTINCT doc.modul FROM iotData WHERE doc.modul IS NOT NULL ORDER BY modul\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all modules\n",
    "spark.sql(\"SELECT DISTINCT doc.modul FROM iotData ORDER BY modul\").show(1000, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sensors in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all sensor names\n",
    "spark.sql(\"SELECT DISTINCT doc.sensor FROM iotData ORDER BY sensor\").show(1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all sensors per module\n",
    "spark.sql(\"SELECT DISTINCT doc.modul, doc.sensor FROM iotData ORDER BY sensor\").show(1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = spark.sql(\"SELECT DISTINCT doc.sensor FROM iotData ORDER BY sensor\")\n",
    "sensors_num = sensor_names.select(\"sensor\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check valid timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_month(date):\n",
    "    valid_months = list(range(1,13,1))\n",
    "    month = dt.datetime.utcfromtimestamp(int(date)/1000).strftime('%m')\n",
    "    if int(month) not in valid_months:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid years are from 2010 to 2025\n",
    "def check_valid_year(date):\n",
    "    valid_years = list(range(2010,2026,1))\n",
    "    year = dt.datetime.utcfromtimestamp(int(date)/1000).strftime('%Y')\n",
    "    if int(year) not in valid_years:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_time(date):\n",
    "    valid_hours = list(range(0,25,1))\n",
    "    valid_minutes = list(range(0,61,1))\n",
    "    valid_seconds = list(range(0,61,1))\n",
    "    hour = dt.datetime.utcfromtimestamp(int(date)/1000).strftime('%H')\n",
    "    minute = dt.datetime.utcfromtimestamp(int(date)/1000).strftime('%M')\n",
    "    second = dt.datetime.utcfromtimestamp(int(date)/1000).strftime('%S')\n",
    "    if int(hour) not in valid_hours:\n",
    "        return False\n",
    "    if int(minute) not in valid_minutes:\n",
    "        return False\n",
    "    if int(second) not in valid_seconds:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = spark.sql(\"SELECT doc.timestamp FROM iotData ORDER BY timestamp\")\n",
    "timestamps_list = timestamps.select(\"timestamp\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unvalid_timestamps = []\n",
    "for timestamp in timestamps_list:\n",
    "    if timestamp != None and timestamp != '0':\n",
    "        if not check_valid_month(timestamp):\n",
    "            unvalid_timestamps.append(timestamp)\n",
    "        if not check_valid_year(timestamp):\n",
    "            unvalid_timestamps.append(timestamp)\n",
    "        if not check_valid_time(timestamp):\n",
    "            unvalid_timestamps.append(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unvalid_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check duplicate sensor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensors = spark.sql(\"SELECT DISTINCT doc.sensor FROM iotData WHERE doc.sensor IS NOT NULL ORDER BY sensor\").collect()\n",
    "sensors = []\n",
    "for i in range(len(df_sensors)):\n",
    "    s = df_sensors[i].asDict()['sensor'].split('.')[::-1][0]\n",
    "    sensors.append(df_sensors[i].asDict()['sensor'].split('.')[::-1])\n",
    "    \n",
    "s_name_1 = [i for i in sensors if len(i) == 1]\n",
    "s_name_2 = [i for i in sensors if len(i) != 1]\n",
    "duplicate_sensor_names = []\n",
    "\n",
    "for i in range(len(s_name_2)):\n",
    "    for j in range(len(s_name_1)):\n",
    "        if s_name_2[i][0] == s_name_1[j][0]:\n",
    "            duplicate_sensor_names.append(['.'.join(s_name_2[i][::-1]),''.join(s_name_1[j])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(duplicate_sensor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis for every duplicated sensor name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(duplicate_sensor_names)):\n",
    "    dp_df = spark.sql(\"SELECT DISTINCT doc.sensor, doc.modul FROM iotData\\\n",
    "                           WHERE doc.sensor='{}'\\\n",
    "                           OR doc.sensor='{}' ORDER BY modul\"\\\n",
    "                          .format(duplicate_sensor_names[i][0], duplicate_sensor_names[i][1])).collect()\n",
    "    \n",
    "    for d in dp_df:    \n",
    "        df_timestamp = spark.sql(\"SELECT MIN(doc.timestamp) AS Min, MAX(doc.timestamp) AS Max, doc.modul, doc.sensor\\\n",
    "            FROM iotData GROUP BY doc.modul, doc.sensor\\\n",
    "            HAVING modul = '{}' AND sensor = '{}'\"\\\n",
    "                                .format(d.asDict()['modul'], d.asDict()['sensor']))\n",
    "        lambda_get_date_utc = udf(lambda x: get_date_utc(x), returnType=StringType())\n",
    "\n",
    "        df_timestamp = df_timestamp.withColumn('Min_utc', lambda_get_date_utc(df_timestamp.Min))\n",
    "        df_timestamp = df_timestamp.withColumn('Max_utc', lambda_get_date_utc(df_timestamp.Max))\n",
    "        df_timestamp.select(\"Min_utc\", \"Max_utc\", \"modul\", \"sensor\").show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis for only one sensor name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'PressPneu'\n",
    "b = 'App.xHL_BG1'\n",
    "df_timestamp = spark.sql(\"SELECT MIN(doc.timestamp) AS Min, MAX(doc.timestamp) AS Max, doc.modul, doc.sensor\\\n",
    "            FROM iotData GROUP BY doc.modul, doc.sensor\\\n",
    "            HAVING modul = '{}' AND sensor = '{}'\"\\\n",
    "                                .format(a, b))\n",
    "lambda_get_date_utc = udf(lambda x: get_date_utc(x), returnType=StringType())\n",
    "\n",
    "df_timestamp = df_timestamp.withColumn('Min_utc', lambda_get_date_utc(df_timestamp.Min))\n",
    "df_timestamp = df_timestamp.withColumn('Max_utc', lambda_get_date_utc(df_timestamp.Max))\n",
    "df_timestamp.select(\"Min_utc\", \"Max_utc\", \"modul\", \"sensor\").show(2, False)\n",
    "\n",
    "b = 'xHL_BG1'\n",
    "df_timestamp = spark.sql(\"SELECT MIN(doc.timestamp) AS Min, MAX(doc.timestamp) AS Max, doc.modul, doc.sensor\\\n",
    "            FROM iotData GROUP BY doc.modul, doc.sensor\\\n",
    "            HAVING modul = '{}' AND sensor = '{}'\"\\\n",
    "                                .format(a, b))\n",
    "lambda_get_date_utc = udf(lambda x: get_date_utc(x), returnType=StringType())\n",
    "\n",
    "df_timestamp = df_timestamp.withColumn('Min_utc', lambda_get_date_utc(df_timestamp.Min))\n",
    "df_timestamp = df_timestamp.withColumn('Max_utc', lambda_get_date_utc(df_timestamp.Max))\n",
    "df_timestamp.select(\"Min_utc\", \"Max_utc\", \"modul\", \"sensor\").show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistical analysis for sensors with numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a stats dataframe for sensor with number values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moduls = spark.sql(\"SELECT DISTINCT doc.modul, doc.sensor \\\n",
    "FROM iotData ORDER BY modul\").collect()\n",
    "\n",
    "schemaStats = StructType([\n",
    "  StructField('count',IntegerType(), True),\n",
    "  StructField('mean',DoubleType(), True),\n",
    "  StructField('stddev',DoubleType(), True),\n",
    "  StructField('min',DoubleType(), True),\n",
    "  StructField('max',DoubleType(), True),\n",
    "  StructField('modul',StringType(), True),\n",
    "  StructField('sensor',StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only sensors with numbers\n",
    "exclude = ['','']\n",
    "\n",
    "df_sensors_num = df.select('doc.sensor','doc.modul')\\\n",
    "    .filter(~col('doc.value')\\\n",
    "    .isin(['true','false']))\\\n",
    "    .createOrReplaceTempView(\"sensors_num\")\n",
    "df_sens = spark.sql(\"SELECT DISTINCT sensor FROM sensors_num ORDER BY sensor\")\n",
    "sensors_num = df_sens.select(\"sensor\").rdd.flatMap(lambda x: x).collect()\n",
    "for e in exclude:\n",
    "    sensors_num.remove(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modules with sensors that have numarical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT modul FROM sensors_num ORDER BY modul\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### statistics per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = spark.createDataFrame([(0, 0.0, 0.0, 0.0, 0.0,\"\",\"\")], schema=schemaStats)\n",
    "\n",
    "for item in tqdm(moduls):\n",
    "    if item.asDict()['sensor'] == 'Temp':\n",
    "#     if item.asDict()['sensor'] in sensors_num:\n",
    "        col_stats = df.filter(df.doc.modul == item.asDict()['modul'])\\\n",
    "            .filter(df.doc.sensor == item.asDict()['sensor'])\\\n",
    "            .filter(~col('doc.value').isin(['true','false']))\\\n",
    "            .describe('doc.value')\n",
    "\n",
    "        df_pd = col_stats.toPandas().set_index(\"summary\").transpose()\n",
    "        df_pd['modul'] = item.asDict()['modul']\n",
    "        df_pd['sensor'] = item.asDict()['sensor']        \n",
    "\n",
    "        newRow = spark.createDataFrame(df_pd)\n",
    "        df_stats = df_stats.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.write.csv('hdfs://192.168.0.10:9000/user/hadoop/Iot_Analytics_Results/sensors_with_numbers_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find outliers in the numerical data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the outlier treshold with the 1.5 times interquartile range\n",
    "def calculate_outlier_thresholds(q25, q75):\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    return cut_off, lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaStats = StructType([\n",
    "  StructField('count',IntegerType(), True),\n",
    "  StructField('mean',DoubleType(), True),\n",
    "  StructField('stddev',DoubleType(), True),\n",
    "  StructField('min',DoubleType(), True),\n",
    "  StructField('max',DoubleType(), True),\n",
    "  StructField('quantil_25',DoubleType(), True),\n",
    "  StructField('quantil_75',DoubleType(), True),\n",
    "  StructField('threshold_lower',DoubleType(), True),\n",
    "  StructField('threshold_upper',DoubleType(), True),\n",
    "  StructField('outliers_num',IntegerType(), True),\n",
    "  StructField('data_len',IntegerType(), True),\n",
    "  StructField('outliers_percentage',DoubleType(), True),\n",
    "  StructField('modul',StringType(), True),\n",
    "  StructField('sensor',StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find outliers for a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df.filter(df.doc.modul == 'Temp')\\\n",
    "    .filter(df.doc.sensor == 'Temp').select('doc.modul','doc.sensor','doc.value')\n",
    "df_sum = df_sum.withColumn(\"value\", df_sum[\"value\"].cast(DoubleType()))\n",
    "\n",
    "q75 = df_sum.approxQuantile(\"value\", [0.75], 0)[0]\n",
    "q25 = df_sum.approxQuantile(\"value\", [0.25], 0)[0]\n",
    "\n",
    "cut_off, lower, upper = calculate_outlier_thresholds(q25, q75)\n",
    "outliers_num = df_sum.filter((df_sum.value < lower) | (df_sum.value > upper)).count()\n",
    "data_len = df_sum.count()\n",
    "outliers_percentage = outliers_num/data_len\n",
    "\n",
    "outliers_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find outliers for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = spark.createDataFrame([(0, 0.0, 0.0, 0.0, 0.0,0.0,0.0,0.0,0.0,0,0,0.0,\"\",\"\")], schema=schemaStats)\n",
    "\n",
    "for item in tqdm(moduls):\n",
    "    if item.asDict()['sensor'] == 'Temp':\n",
    "#     if item.asDict()['sensor'] in sensors_num:\n",
    "        col_stats = df.filter(df.doc.modul == item.asDict()['modul'])\\\n",
    "            .filter(df.doc.sensor == item.asDict()['sensor'])\\\n",
    "            .filter(~col('doc.value').isin(['true','false']))\\\n",
    "            .describe('doc.value')\n",
    "\n",
    "        df_sum = df.filter(df.doc.modul == item.asDict()['modul'])\\\n",
    "                .filter(df.doc.sensor == item.asDict()['sensor']).select('doc.value')\n",
    "        df_sum = df_sum.withColumn(\"value\", df_sum[\"value\"].cast(DoubleType()))\n",
    "        q75 = df_sum.approxQuantile(\"value\", [0.75], 0)[0]\n",
    "        q25 = df_sum.approxQuantile(\"value\", [0.25], 0)[0]\n",
    "        cut_off, lower, upper = calculate_outlier_thresholds(q25, q75)\n",
    "        outliers_num = df_sum.filter((df_sum.value < lower) | (df_sum.value > upper)).count()\n",
    "        data_len = df_sum.count()\n",
    "        outliers_percentage = outliers_num/data_len\n",
    "\n",
    "        df_pd = col_stats.toPandas().set_index(\"summary\").transpose()\n",
    "        df_pd['quantil_25'] = q25\n",
    "        df_pd['quantil_75'] = q75\n",
    "        df_pd['threshold_lower'] = lower\n",
    "        df_pd['threshold_upper'] = upper\n",
    "        df_pd['outliers_num'] = outliers_num\n",
    "        df_pd['data_len'] = data_len\n",
    "        df_pd['outliers_percentage'] = outliers_percentage\n",
    "        df_pd['modul'] = item.asDict()['modul']\n",
    "        df_pd['sensor'] = item.asDict()['sensor']        \n",
    "\n",
    "        newRow = spark.createDataFrame(df_pd)\n",
    "        df_stats = df_stats.union(newRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Null-Values per Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM df WHERE (iCarrierID IS NOT NULL) AND value IS NOT NULL\").show(1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    sql_string = \"SELECT COUNT(*) - COUNT({}) AS NULL_VALUES, COUNT({}) FROM df\".format(col, col)\n",
    "    spark.sql(sql_string).show(20, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
